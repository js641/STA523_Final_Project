myCorpus = tm_map(myCorpus, content_transformer(tolower))
myCorpus = tm_map(myCorpus, content_transformer(removePunctuation))
myCorpus = tm_map(myCorpus, content_transformer(removeNumbers))
removeURL = function(x) gsub("http[[:alnum:]]*", "", x)
myCorpus = tm_map(myCorpus, content_transformer(removeURL))
exclude_words=c("http", "rt", "co", "in", "of", "is", "you", "me", "my", "mine", "to", "the", "i", "them", "so", "t", "by", "?", "it",
"so", "continue", "will", "probably", "was", "one", "two", "aboard", "about", "above", "across", "after", "against",
"along", "amid", "among", "anti", "around", "as", "at", "before", "behind", "below", "beneath", "beside", "besides",
"between", "btw", "beyond", "but", "by", "concerning", "considering", "despite", "down", "during", "except", "excepting",
"excluding", "following", "for", "from", "in", "inside", "into", "like", "minus", "near", "of", "off", "on", "onto",
"opposite", "outside", "over", "past", "per", "plus", "regarding", "round", "since", "than", "through", "to", "toward",
"towards", "under", "underneath", "unlike", "until", "up", "upon", "versus", "via", "with", "within", "without", "?",
"!", "?", "out", "it", "as", "when", "will", "not", "probably", "was", "have", "has", "this", "that", "a", "for",
"htt", "https", "many", "we", "st", "if", "ok", "okay", "all", "and", "just", "did", "amp", "what", "your", "or", "either",
"k", "ain", "ain't", "here", "are", "there", "their", "htt?", "need", "basically", "way", "why", "who", "thru",
"can", "be", "get", "today", "guy", "day", "help", "time", "tomorrow", "tonight", "now", "try", "please")
myStopwords = c(stopwords("english"), "isis")
myStopwords = unique(c(myStopwords,exclude_words))
myCorpus = tm_map(myCorpus, removeWords, myStopwords)
myCorpusCopy <- myCorpus
myCorpus = tm_map(myCorpus, content_transformer(stemDocument))
for(i in 1:5) {
cat(paste("[[", i, "]]", sep = ""))
writeLines(as.character(myCorpus[[i]]))
}
?setdiff
(x <- c(sort(sample(1:20, 9)), NA))
(y <- c(sort(sample(3:23, 7)), NA))
union(x, y)
intersect(x, y)
setdiff(x, y)
setdiff(y, x)
setequal(x, y)
myStopwords = c(setdiff(myStopwords,exclude_words),setdiff(exclude_words,myStopwords))
myStopwords = c(stopwords("english"), "isis")
myStopwords = union(myStopwords,exclude_words)
myCorpus = tm_map(myCorpus, removeWords, myStopwords)
myCorpusCopy <- myCorpus
myCorpus = tm_map(myCorpus, content_transformer(stemDocument))
for(i in 1:5) {
cat(paste("[[", i, "]]", sep = ""))
writeLines(as.character(myCorpus[[i]]))
}
duplicated(myStopwords)
myCorpus = Corpus(VectorSource(tweets.df$text))
myCorpus = tm_map(myCorpus, content_transformer(tolower))
myCorpus = tm_map(myCorpus, content_transformer(removePunctuation))
myCorpus = tm_map(myCorpus, content_transformer(removeNumbers))
removeURL = function(x) gsub("http[[:alnum:]]*", "", x)
myCorpus = tm_map(myCorpus, content_transformer(removeURL))
exclude_words=c("http", "rt", "co", "in", "of", "is", "you", "me", "my", "mine", "to", "the", "i", "them", "so", "t", "by", "?", "it",
"so", "continue", "will", "probably", "was", "one", "two", "aboard", "about", "above", "across", "after", "against",
"along", "amid", "among", "anti", "around", "as", "at", "before", "behind", "below", "beneath", "beside", "besides",
"between", "btw", "beyond", "but", "by", "concerning", "considering", "despite", "down", "during", "except", "excepting",
"excluding", "following", "for", "from", "in", "inside", "into", "like", "minus", "near", "of", "off", "on", "onto",
"opposite", "outside", "over", "past", "per", "plus", "regarding", "round", "since", "than", "through", "to", "toward",
"towards", "under", "underneath", "unlike", "until", "up", "upon", "versus", "via", "with", "within", "without", "?",
"!", "?", "out", "it", "as", "when", "will", "not", "probably", "was", "have", "has", "this", "that", "a", "for",
"htt", "https", "many", "we", "st", "if", "ok", "okay", "all", "and", "just", "did", "amp", "what", "your", "or", "either",
"k", "ain", "ain't", "here", "are", "there", "their", "htt?", "need", "basically", "way", "why", "who", "thru",
"can", "be", "get", "today", "guy", "day", "help", "time", "tomorrow", "tonight", "now", "try", "please")
myStopwords = c(stopwords("english"), "isis")
myStopwords = union(myStopwords,exclude_words)
myCorpus = tm_map(myCorpus, removeWords, myStopwords)
myCorpusCopy <- myCorpus
myCorpus = tm_map(myCorpus, content_transformer(stemDocument))
for(i in 1:5) {
cat(paste("[[", i, "]]", sep = ""))
writeLines(as.character(myCorpus[[i]]))
}
exclude_words=c("http", "rt", "co", "in", "of", "is", "you", "me", "my", "mine", "to", "the", "i", "them", "so", "t", "by", "it",
"so", "continue", "will", "probably", "was", "one", "two", "aboard", "about", "above", "across", "after", "against",
"along", "amid", "among", "anti", "around", "as", "at", "before", "behind", "below", "beneath", "beside", "besides",
"between", "btw", "beyond", "but", "by", "concerning", "considering", "despite", "down", "during", "except", "excepting",
"excluding", "following", "for", "from", "in", "inside", "into", "like", "minus", "near", "of", "off", "on", "onto",
"opposite", "outside", "over", "past", "per", "plus", "regarding", "round", "since", "than", "through", "to", "toward",
"towards", "under", "underneath", "unlike", "until", "up", "upon", "versus", "via", "with", "within", "without",
"out", "it", "as", "when", "will", "not", "probably", "was", "have", "has", "this", "that", "a", "for",
"htt", "https", "many", "we", "st", "if", "ok", "okay", "all", "and", "just", "did", "amp", "what", "your", "or", "either",
"k", "ain", "ain't", "here", "are", "there", "their", "htt?", "need", "basically", "way", "why", "who", "thru",
"can", "be", "get", "today", "guy", "day", "help", "time", "tomorrow", "tonight", "now", "try", "please")
myStopwords = c(stopwords("english"), "isis")
myStopwords = union(myStopwords,exclude_words)
myCorpus = tm_map(myCorpus, removeWords, myStopwords)
myCorpusCopy <- myCorpus
myCorpus = tm_map(myCorpus, content_transformer(stemDocument))
for(i in 1:5) {
cat(paste("[[", i, "]]", sep = ""))
writeLines(as.character(myCorpus[[i]]))
}
n.tweet = length(tweets)
tweets[1:5]
tweets.df = twListToDF(tweets)
tweets.df$text=str_replace_all(tweets.df$text,"[^[:graph:]]", " ")
myCorpus = Corpus(VectorSource(tweets.df$text))
myCorpus = tm_map(myCorpus, content_transformer(tolower))
myCorpus = tm_map(myCorpus, content_transformer(removePunctuation))
myCorpus = tm_map(myCorpus, content_transformer(removeNumbers))
removeURL = function(x) gsub("http[[:alnum:]]*", "", x)
myCorpus = tm_map(myCorpus, content_transformer(removeURL))
myStopwords = c(stopwords("english"), "isis","rt","amp")
#myStopwords = union(myStopwords,exclude_words)
myCorpus = tm_map(myCorpus, removeWords, myStopwords)
myCorpusCopy <- myCorpus
myCorpus = tm_map(myCorpus, content_transformer(stemDocument))
for(i in 1:5) {
cat(paste("[[", i, "]]", sep = ""))
writeLines(as.character(myCorpus[[i]]))
}
#myCorpus <- tm_map(myCorpus, content_transformer(stemCompletion), dictionary = myCorpusCopy, lazy=TRUE)
tdm <- TermDocumentMatrix(myCorpus, control = list(wordLengths = c(1, Inf)))
tdm
idx = which(dimnames(tdm)$Terms == "family")
inspect(tdm[idx+(0:5), 101:110])
(freq.terms = findFreqTerms(tdm, lowfreq = 15))
term.freq = rowSums(as.matrix(tdm))
term.freq = subset(term.freq, term.freq >= 15)
df = data.frame(term = names(term.freq), freq = term.freq)
library(ggplot2)
ggplot(df, aes(x = term, y = freq)) + geom_bar(stat = "identity") + xlab("Terms") + ylab("Count") + coord_flip()
findAssocs(tdm, "r", 0.2)
findAssocs(tdm, "mining", 0.25)
# http://www.bioconductor.org/packages/release/bioc/html/graph.html
#source("https://bioconductor.org/biocLite.R")
#biocLite("graph")
library(graph)
#install.packages("wordcloud")
library(wordcloud)
m = as.matrix(tdm)
word.freq = sort(rowSums(m), decreasing = T)
wordcloud(words = names(word.freq),random.color = TRUE, colors=rainbow(10), freq = word.freq, min.freq = 10, random.order = F)
shiny::runApp('Dropbox/STA523/FinalProject')
shiny::runApp('Dropbox/STA523/FinalProject')
shiny::runApp('Dropbox/STA523/FinalProject')
shiny::runApp('Dropbox/STA523/FinalProject')
shiny::runApp('Dropbox/STA523/FinalProject')
shiny::runApp('Dropbox/STA523/FinalProject')
## Now we load the packages
EnsurePackage<-function(x)
{x <- as.character(x)
if (!require(x,character.only=TRUE))
{
install.packages(pkgs=x,repos="http://cran.r-project.org")
require(x,character.only=TRUE)
}
}
#Identifying packages required
PrepareTwitter<-function()
{
EnsurePackage("httr")
EnsurePackage("devtools")
EnsurePackage("twitteR")
EnsurePackage("base64enc")
EnsurePackage("stringr")
EnsurePackage("ROAuth")
EnsurePackage("RCurl")
EnsurePackage("ggplot2")
EnsurePackage("tm")
EnsurePackage("RJSONIO")
EnsurePackage("wordcloud")
EnsurePackage("ggplot2")
EnsurePackage("streamR")
EnsurePackage("NYU160J")
EnsurePackage("grid")
}
PrepareTwitter()
shiny::runApp('Dropbox/STA523/FinalProject')
shiny::runApp('Dropbox/STA523/FinalProject')
shiny::runApp('Dropbox/STA523/FinalProject')
?observe
shiny::runApp('Dropbox/STA523/FinalProject')
?searchTwitter
tweets = searchTwitter(n = 3200, lang="en")
tweets = searchTwitter("",n = 3200, lang="en")
shiny::runApp('Dropbox/STA523/FinalProject')
shiny::runApp('Dropbox/STA523/FinalProject')
shiny::runApp('Dropbox/STA523/FinalProject')
shiny::runApp('Dropbox/STA523/FinalProject')
shiny::runApp('Dropbox/STA523/FinalProject')
library(magrittr)
myCorpus = Corpus(VectorSource(tweets.df$text)) %>%
tm_map(content_transformer(tolower))
n.tweet = length(tweets)
tweets[1:5]
tweets.df = twListToDF(tweets)
tweets.df$text=str_replace_all(tweets.df$text,"[^[:graph:]]", " ")
library(magrittr)
myCorpus = Corpus(VectorSource(tweets.df$text)) %>%
tm_map(content_transformer(tolower))
tweets = searchTwitter("ISIS",n = 3200, lang="en")
library(tm)
library(SnowballC)
library(stringr)
n.tweet = length(tweets)
tweets[1:5]
tweets.df = twListToDF(tweets)
tweets.df$text=str_replace_all(tweets.df$text,"[^[:graph:]]", " ")
library(magrittr)
myCorpus = Corpus(VectorSource(tweets.df$text)) %>%
tm_map(content_transformer(tolower)) %>%
tm_map(content_transformer(removePunctuation))
myCorpus = Corpus(VectorSource(tweets.df$text)) %>%
tm_map(content_transformer(tolower)) %>%
tm_map(content_transformer(removePunctuation)) %>%
tm_map(content_transformer(removeNumbers)) %>%
tm_map(content_transformer(removeURL)) %>%
tm_map(removeWords, myStopwords)
removeURL = function(x) gsub("http[[:alnum:]]*", "", x)
myStopwords = c(stopwords("english"), "isis","rt","amp","twitter", "tweets", "tweet", "retweet",
"tweeting", "account", "via", "cc", "ht")
myCorpus = Corpus(VectorSource(tweets.df$text)) %>%
tm_map(content_transformer(tolower)) %>%
tm_map(content_transformer(removePunctuation)) %>%
tm_map(content_transformer(removeNumbers)) %>%
tm_map(content_transformer(removeURL)) %>%
tm_map(removeWords, myStopwords)
myCorpusCopy <- myCorpus
myCorpus = tm_map(myCorpus, content_transformer(stemDocument))
for(i in 1:5) {
cat(paste("[[", i, "]]", sep = ""))
writeLines(as.character(myCorpus[[i]]))
}
tdm <- TermDocumentMatrix(myCorpus, control = list(wordLengths = c(1, Inf)))
tdm <- TermDocumentMatrix(myCorpus, control = list(wordLengths = c(1, Inf)))
m = as.matrix(tdm)
word.freq = sort(rowSums(m), decreasing = T)
wordcloud(words = names(word.freq),random.color = TRUE, colors=rainbow(10), freq = word.freq, min.freq = 10, random.order = F)
shiny::runApp('Dropbox/STA523/FinalProject')
shiny::runApp('Dropbox/STA523/FinalProject')
shiny::runApp('Dropbox/STA523/FinalProject')
shiny::runApp('Dropbox/STA523/FinalProject')
shiny::runApp('Dropbox/STA523/FinalProject')
shiny::runApp('Dropbox/STA523/FinalProject')
tweets.df[,1]
shiny::runApp('Dropbox/STA523/FinalProject')
?strip_retweets
shiny::runApp('Dropbox/STA523/FinalProject')
shiny::runApp('Dropbox/STA523/FinalProject')
shiny::runApp('Dropbox/STA523/FinalProject')
shiny::runApp('Dropbox/STA523/FinalProject')
shiny::runApp('Dropbox/STA523/FinalProject')
shiny::runApp('Dropbox/STA523/FinalProject')
shiny::runApp('Dropbox/STA523/FinalProject')
shiny::runApp('Dropbox/STA523/FinalProject')
shinyApp(ui = ui, server = server)
shiny::runApp('Dropbox/STA523/FinalProject')
## Now we load the packages
EnsurePackage<-function(x)
{x <- as.character(x)
if (!require(x,character.only=TRUE))
{
install.packages(pkgs=x,repos="http://cran.r-project.org")
require(x,character.only=TRUE)
}
}
#Identifying packages required
PrepareTwitter<-function()
{
EnsurePackage("httr")
EnsurePackage("devtools")
EnsurePackage("twitteR")
EnsurePackage("base64enc")
EnsurePackage("stringr")
EnsurePackage("ROAuth")
EnsurePackage("RCurl")
EnsurePackage("ggplot2")
EnsurePackage("tm")
EnsurePackage("RJSONIO")
EnsurePackage("wordcloud")
EnsurePackage("ggplot2")
EnsurePackage("streamR")
EnsurePackage("NYU160J")
EnsurePackage("grid")
}
PrepareTwitter()
shiny::runApp('Dropbox/STA523/FinalProject')
shiny::runApp('Dropbox/STA523/FinalProject')
shiny::runApp('Dropbox/STA523/FinalProject')
shiny::runApp('Dropbox/STA523/FinalProject')
n.tweet = length(tweets)
tweets[1:5]
tweets.df = twListToDF(tweets)
tweets[1:5]
tweets.df = do.call("rbind",lapply(tweets,as.data.frame))
View(tweets.df)
shiny::runApp('Dropbox/STA523/FinalProject')
shiny::runApp('Dropbox/STA523/FinalProject')
shiny::runApp('Dropbox/STA523/FinalProject')
shiny::runApp('Dropbox/STA523/FinalProject')
shiny::runApp('Dropbox/STA523/FinalProject')
shiny::runApp('Dropbox/STA523/FinalProject')
shiny::runApp('Dropbox/STA523/FinalProject')
shiny::runApp('Dropbox/STA523/FinalProject')
## Now we load the packages
EnsurePackage<-function(x)
{x <- as.character(x)
if (!require(x,character.only=TRUE))
{
install.packages(pkgs=x,repos="http://cran.r-project.org")
require(x,character.only=TRUE)
}
}
#Identifying packages required
PrepareTwitter<-function()
{
EnsurePackage("httr")
EnsurePackage("devtools")
EnsurePackage("twitteR")
EnsurePackage("base64enc")
EnsurePackage("stringr")
EnsurePackage("ROAuth")
EnsurePackage("RCurl")
EnsurePackage("ggplot2")
EnsurePackage("tm")
EnsurePackage("RJSONIO")
EnsurePackage("wordcloud")
EnsurePackage("ggplot2")
EnsurePackage("streamR")
EnsurePackage("NYU160J")
EnsurePackage("grid")
}
PrepareTwitter()
shiny::runApp('Dropbox/STA523/FinalProject')
## Now we load the packages
EnsurePackage<-function(x)
{x <- as.character(x)
if (!require(x,character.only=TRUE))
{
install.packages(pkgs=x,repos="http://cran.r-project.org")
require(x,character.only=TRUE)
}
}
#Identifying packages required
PrepareTwitter<-function()
{
EnsurePackage("httr")
EnsurePackage("devtools")
EnsurePackage("twitteR")
EnsurePackage("base64enc")
EnsurePackage("stringr")
EnsurePackage("ROAuth")
EnsurePackage("RCurl")
EnsurePackage("ggplot2")
EnsurePackage("tm")
EnsurePackage("RJSONIO")
EnsurePackage("wordcloud")
EnsurePackage("ggplot2")
EnsurePackage("streamR")
EnsurePackage("NYU160J")
EnsurePackage("grid")
}
PrepareTwitter()
shiny::runApp('Dropbox/STA523/FinalProject')
## Now we load the packages
EnsurePackage<-function(x)
{x <- as.character(x)
if (!require(x,character.only=TRUE))
{
install.packages(pkgs=x,repos="http://cran.r-project.org")
require(x,character.only=TRUE)
}
}
#Identifying packages required
PrepareTwitter<-function()
{
EnsurePackage("httr")
EnsurePackage("devtools")
EnsurePackage("twitteR")
EnsurePackage("base64enc")
EnsurePackage("stringr")
EnsurePackage("ROAuth")
EnsurePackage("RCurl")
EnsurePackage("ggplot2")
EnsurePackage("tm")
EnsurePackage("RJSONIO")
EnsurePackage("wordcloud")
EnsurePackage("ggplot2")
EnsurePackage("streamR")
EnsurePackage("NYU160J")
EnsurePackage("grid")
}
PrepareTwitter()
shiny::runApp('Dropbox/STA523/FinalProject')
## Now we load the packages
EnsurePackage<-function(x)
{x <- as.character(x)
if (!require(x,character.only=TRUE))
{
install.packages(pkgs=x,repos="http://cran.r-project.org")
require(x,character.only=TRUE)
}
}
#Identifying packages required
PrepareTwitter<-function()
{
EnsurePackage("httr")
EnsurePackage("devtools")
EnsurePackage("twitteR")
EnsurePackage("base64enc")
EnsurePackage("stringr")
EnsurePackage("ROAuth")
EnsurePackage("RCurl")
EnsurePackage("ggplot2")
EnsurePackage("tm")
EnsurePackage("RJSONIO")
EnsurePackage("wordcloud")
EnsurePackage("ggplot2")
EnsurePackage("streamR")
EnsurePackage("NYU160J")
EnsurePackage("grid")
}
PrepareTwitter()
install.packages("fpc")
library(fpc)
install.packages("topicmodels")
library(topicmodels)
install.packages("mime")
library(mime)
shiny::runApp('Dropbox/STA523/FinalProject')
tdm2=removeSparseTerms(tdm, sparse = 0.95)
library(twitteR)
consumer_key<-"zSeAWHNpaL5G7GpHuCO4zTffT"
consumer_secret<-"dZnarPgWiQCnb1bJ0tvN3xFBmWVMRQCDDWl9UsFtkiTGpzztfG"
access_token<-"1071126529-DLvrKbaT9ju1yQsAHBWZz5h3vHGEWWyWYeTHP4Z"
access_secret<-"h3XgPKhsKkF66ShXfbFPEnby2VUofGD6AYvu53CFvUFX7"
requestURL <- "https://api.twitter.com/oauth/request_token"
accessURL <- "https://api.twitter.com/oauth/access_token"
authURL <- "https://api.twitter.com/oauth/authorize"
setup_twitter_oauth(consumer_key, consumer_secret, access_token,access_secret)
##Seach for random Topic
tweets = searchTwitter("ISIS",n = 1000, lang="en")
##Search for users use
#install.packages("tm")
#install.packages("SnowballC")
library(tm)
library(SnowballC)
library(stringr)
n.tweet = length(tweets)
tweets[1:5]
tweets.df = twListToDF(tweets)
tweets.df$text=str_replace_all(tweets.df$text,"[^[:graph:]]", " ")
library(magrittr)
removeURL = function(x) gsub("http[[:alnum:]]*", "", x)
myStopwords = c(stopwords("english"), "isis","rt","amp","twitter", "tweets", "tweet", "retweet",
"tweeting", "account", "via", "cc", "ht")
myCorpus = Corpus(VectorSource(tweets.df$text)) %>%
tm_map(content_transformer(tolower)) %>%
tm_map(content_transformer(removePunctuation)) %>%
tm_map(content_transformer(removeNumbers)) %>%
tm_map(content_transformer(removeURL)) %>%
tm_map(removeWords, myStopwords) %>%
tm_map(content_transformer(stemDocument))
for(i in 1:5) {
cat(paste("[[", i, "]]", sep = ""))
writeLines(as.character(myCorpus[[i]]))
}
#myCorpus <- tm_map(myCorpus, content_transformer(stemCompletion), dictionary = myCorpusCopy, lazy=TRUE)
tdm <- TermDocumentMatrix(myCorpus, control = list(wordLengths = c(1, Inf)))
m = as.matrix(tdm)
word.freq = sort(rowSums(m), decreasing = T)
wordcloud(words = names(word.freq),random.color = TRUE, colors=rainbow(10), freq = word.freq, min.freq = 10, random.order = F)
#idx = which(dimnames(tdm)$Terms == "family")
#inspect(tdm[idx+(0:5), 101:110])
freq.terms <- findFreqTerms(tdm, lowfreq=40)
term.freq = rowSums(as.matrix(tdm))
term.freq = subset(term.freq, term.freq >= 80)
df = data.frame(term = names(term.freq), freq = term.freq)
library(ggplot2)
ggplot(df, aes(x = term, y = freq)) + geom_bar(stat = "identity") + xlab("Terms") + ylab("Count") + coord_flip()
us=as.data.frame(findAssocs(tdm, "us", 0.2))
findAssocs(tdm, "obama", 0.25)
# http://www.bioconductor.org/packages/release/bioc/html/graph.html
#source("https://bioconductor.org/biocLite.R")
#biocLite("graph")
library(graph)
library(Rgraphviz)
plot(tdm, term = freq.terms, corThreshold = 0.12, weighting = T)
library(wordcloud)
m = as.matrix(tdm)
word.freq = sort(rowSums(m), decreasing = T)
wordcloud(words = names(word.freq),random.color = TRUE, colors=rainbow(10), freq = word.freq, min.freq = 10, random.order = F)
shiny::runApp('Dropbox/STA523/FinalProject')
shiny::runApp('Dropbox/STA523/FinalProject')
shiny::runApp('Dropbox/STA523/FinalProject')
shiny::runApp('Dropbox/STA523/FinalProject')
tdm2=removeSparseTerms(tdm, sparse = 0.95)
m2=as.matrix(tdm2)
distMatrix=dist(scale(m2))
fit=hclust(distMatrix, method="ward")
plot(fit)
rect.hclust(fit,k = 6) # cut tree into 6 clusters
m3=t(m2) # transpose the matrix to cluster documents(tweets)
set.seed(122) # set a fixed random seed
k=6 # number of clusters
kmeansResult=kmeans(m3,k)
round(kmeansResult$centers, digits = 3) # cluster centers
for (i in 1:k) {
cat(paste("cluster ",i,":  ",sep=""))
s=sort(kmeansResult$centers[i, ], decreasing = T)
cat(names(s)[1:5], "\n")
#print the tweets of every clusters
#print (tweets[which(kmeansResult$clusters==i)])
}
pamResult=pamk(m3, metric="manhanttan")
k=pamResult$nc #number of clusters identified
pamResult=pamResult$pamobject
shiny::runApp('Dropbox/STA523/FinalProject')
shiny::runApp('Dropbox/STA523/FinalProject')
shiny::runApp('Dropbox/STA523/FinalProject')
shiny::runApp('Dropbox/STA523/FinalProject')
shiny::runApp('Dropbox/STA523/FinalProject')
shiny::runApp('Dropbox/STA523/FinalProject')
